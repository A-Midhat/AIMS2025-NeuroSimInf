{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation and Inference for Neuroscience\n",
    "## Exercises Week 3: Parameter inference for simulators in neuroscience\n",
    "\n",
    "The notebook below contains the exercises for week 3.\n",
    "The exercises are designed to follow the lecture material and do not need deep knowledge of python to be solved.\n",
    "\n",
    "There are two types of exercises:\n",
    "- **Tasks**: Where you need to write code, i.e. \"implement a function that does X\".\n",
    "- **Questions**: Where you need to answer questions using the code you wrote. For example \"vary the inputs and interpret the results\", will require you to provide one or more plots together with your answer. To answer the questions you can use Markdown cells.\n",
    "\n",
    "All necessary dependencies are imported in the first cell and you should not need to import anything else. In addition, we also provide some helper functions that can be used to help you answering the tasks. When solving the tasks, feel free to change the provided function signatures and code as you see fit. However, we recommend sticking with the provided signatures as they reflect common conventions and will make it easier to add additional functionality during the exercise. Type hints are provided for the function signatures and the arguments and each task should only require a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure jax to use 64bit precision and cpu\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the project root to the python path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# import the necessary dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import torch\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit, vmap, value_and_grad\n",
    "\n",
    "# jaxley imports\n",
    "import jaxley as jx\n",
    "from jaxley.channels import HH\n",
    "from jaxley.stimulus import step_current\n",
    "from jaxley.optimize.utils import l2_norm\n",
    "from jaxley.optimize.transforms import SigmoidTransform, ParamTransform\n",
    "\n",
    "# sbi imports\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.inference import SNPE\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "# Typing imports\n",
    "from typing import Callable,Union, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def plot_observation(\n",
    "    t_obs: Union[jnp.ndarray, torch.tensor],\n",
    "    v_obs: Union[jnp.ndarray, torch.tensor],\n",
    "    i_obs: Optional[Union[jnp.ndarray, torch.tensor]] = None,\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot the observed voltage and stimulus.\n",
    "\n",
    "    If stimulus is provided it will create a figure with 2 axes, otherwise it will create a figure with 1 axis.\n",
    "\n",
    "    Args:\n",
    "        t_obs: The time points of the observed voltage and stimulus (ms).\n",
    "        v_obs: The observed voltage (mV).\n",
    "        i_obs: The observed stimulus (nA).\n",
    "\n",
    "    Returns:\n",
    "        fig: The figure.\n",
    "        ax: The axes.\n",
    "    \"\"\"\n",
    "    num_rows = 2 if i_obs is not None else 1\n",
    "    height_ratios = [3, 1] if i_obs is not None else [1]\n",
    "    fig, ax = plt.subplots(\n",
    "        num_rows,\n",
    "        1,\n",
    "        figsize=(10, 5),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": height_ratios},\n",
    "    )\n",
    "    if num_rows == 1:\n",
    "        ax = [ax]\n",
    "    ax[0].plot(t_obs, v_obs, c=\"k\")\n",
    "    ax[0].set_xlabel(\"t (ms)\")\n",
    "    ax[0].set_ylabel(\"V (mV)\")\n",
    "    if i_obs is not None:\n",
    "        ax[1].plot(t_obs, i_obs, c=\"k\")\n",
    "        ax[1].set_xlabel(\"t (ms)\")\n",
    "        ax[1].set_ylabel(\"stimulus (nA)\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def make_plot(\n",
    "    t_est: Union[jnp.ndarray, torch.tensor],\n",
    "    v_est: Union[jnp.ndarray, torch.tensor],\n",
    "    t_obs: Optional[Union[jnp.ndarray, torch.tensor]] = None,\n",
    "    v_obs: Optional[Union[jnp.ndarray, torch.tensor]] = None,\n",
    "    i_obs: Optional[Union[jnp.ndarray, torch.tensor]] = None,\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"Plot the estimated voltage and stimulus (along with the observed data if provided).\n",
    "\n",
    "    Args:\n",
    "        t_est: The time points of the estimated voltage (ms).\n",
    "        v_est: The estimated voltage (mV).\n",
    "        t_obs: The time points of the observed voltage (ms).\n",
    "        v_obs: The observed voltage (mV).\n",
    "        i_obs: The observed stimulus (nA).\n",
    "    \"\"\"\n",
    "    if t_obs is not None:\n",
    "        fig, ax = plot_observation(t_obs, v_obs, i_obs)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        ax = [ax]\n",
    "    ax[0].plot(t_est, v_est, c=\"r\", alpha=0.5)\n",
    "    ax[0].set_xlabel(\"t (ms)\")\n",
    "    ax[0].set_ylabel(\"V (mV)\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer parameters of a single compartment HH model\n",
    "\n",
    "In week 1 we have learned how to simulate neural dynamics using the Hodgkin-Huxley model, i.e. going from parameters to observations. We call this the forward problem. \n",
    "In week 2 on the otherhand, you were introduced to methods that solve the so called inverse problem, i.e. going from observations to parameters. \n",
    "Now we will put these two together and see how we can combine simulation and inference methods to infer the parameters of a Hodgkin-Huxley model from a measured voltage trace.\n",
    "\n",
    "Lets start by importing and looking at the data, before attempting to infer the model parameters at different levels of sophistication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observation (note that we convert the data to a jax array)\n",
    "t_obs, v_obs, stim_obs = jnp.array(np.loadtxt(\"../data/observation.csv\", unpack=True))\n",
    "\n",
    "# stimulus information\n",
    "sampling_interval = t_obs[1] - t_obs[0]\n",
    "stim_amp = jnp.max(stim_obs)\n",
    "stim_onset = t_obs[stim_obs.nonzero()][0]\n",
    "stim_end = t_obs[stim_obs.nonzero()][-1] + sampling_interval\n",
    "stim_dur = stim_end - stim_onset\n",
    "t_max = t_obs[-1] + sampling_interval\n",
    "\n",
    "print(\"Sampling interval: \", sampling_interval, \"ms\")\n",
    "print(\"Max. time: \", t_max, \"ms\")\n",
    "print(\"Stimulus amplitude: \", stim_amp, \"nA\")\n",
    "print(\"Stimulus (end | duration | onset): \", f\"{stim_end:.2f} | {stim_dur:.2f} | {stim_onset:.2f} ms\")\n",
    "\n",
    "# plot the observation\n",
    "plot_observation(t_obs, v_obs, i_obs=stim_obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-tuning\n",
    "Arguably the most simple approach to solve the inverse problem is just by \"trial and error\" or hand tuning the parameters. How difficult this can already be for just 3 parameters, we will explore in the following:\n",
    "\n",
    "### Tasks\n",
    "- Implement a simulator for a single compartment HH model in `jaxley`\n",
    "    - For details see the [documentation](https://jaxley.readthedocs.io/en/latest/index.html)\n",
    "- hand-tune (manually vary) the different conductances $g_{Na}$, $g_{K}$, $g_{leak}$ to fit a single compartment HH model to the measured voltage trace.\n",
    "- plot the results\n",
    "\n",
    "### Questions\n",
    "- How do you assess the quality of the fit, i.e. what (characteristics) do you look for?\n",
    "- What are ways to quantify this?\n",
    "\n",
    "_Note: don't forget to set the initial conditions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the simulation\n",
    "\n",
    "# NOTE: Keep in mind that the stepsize of the solver is 1/10th of the sampling interval of our experimental data!\n",
    "dt = 0.025\n",
    "\n",
    "# define the stimulus\n",
    "stimulus = step_current(\n",
    "    # add the stimulus parameters here\n",
    ")\n",
    "ts = jnp.arange(\n",
    "    # add the time points here\n",
    ")\n",
    "\n",
    "# create a single compartment HH model, add a stimulus and record the voltage\n",
    "\n",
    "comp = # define the model here \n",
    "# add a recording and stimulation here ()\n",
    "\n",
    "# set the initial conditions\n",
    "# initialize the model\n",
    "\n",
    "# define a simulator\n",
    "@jit\n",
    "def simulate(params: dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"Simulate the HH model for the given parameters.\n",
    "\n",
    "    Note that we are using `@jit` to compile the function before running it.\n",
    "    Since we are running the function a lot of times, it is worth the addtional overhead \n",
    "    to compile it first.\n",
    "    \n",
    "    Args:\n",
    "        params: The parameters of the HH model.\n",
    "\n",
    "    Returns:\n",
    "        v: The voltage trace.\n",
    "    \"\"\"\n",
    "    # set the simulation parameters to update the model\n",
    "    pstate = None\n",
    "    for key, value in params.items():\n",
    "        pstate = comp.data_set(key, value, pstate)\n",
    "\n",
    "    # integrate the model\n",
    "    v = jx.integrate(comp, param_state=pstate, delta_t=dt)\n",
    "    return v[:, :len(ts)] # match length to ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter bounds\n",
    "lb = {\"HH_gNa\": 1e-3, \"HH_gK\": 1e-3, \"HH_gLeak\": 1e-5}\n",
    "ub = {\"HH_gNa\": 1.0, \"HH_gK\": 1.0, \"HH_gLeak\": 1e-3}\n",
    "bounds = {k:(lb,ub) for k, (lb, ub) in zip(lb.keys(), zip(lb.values(), ub.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-tune the parameters\n",
    "param_guess = {\"HH_gNa\": jnp.array([0.3]), \"HH_gK\": jnp.array([0.2]), \"HH_gLeak\": jnp.array([0.0001])}\n",
    "v_guess = simulate(param_guess)\n",
    "\n",
    "# plot the voltage trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random / Grid search\n",
    "While hand-tuning is a very simple method that can work for a few parameters, you should have noticed that this is already tricky for just 3 parameters and it will quickly become infeasible for more complex models and larger parameter spaces. Therefore we will explore a still simple, but more systematic approach: random / grid search.\n",
    "\n",
    "### Tasks\n",
    "- Implement a mean absolute error loss\n",
    "- Implement **either** a random **or** a grid search to find suitable parameters\n",
    "- Plot your top 10 results as quantified by their mean absolute error\n",
    "\n",
    "### Questions\n",
    "- Are the results what you expected? Why / Why not?\n",
    "- What are different ways in which you could improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize the simulation (for more details see https://docs.jax.dev/en/latest/_autosummary/jax.vmap.html)\n",
    "# this way you can input an entire batch of parameters rather than having to loop over them\n",
    "parallel_simulate = vmap(simulate)\n",
    "\n",
    "# define an error / distance function\n",
    "def mean_abs_err(v_est: jnp.ndarray, v_true: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Compute the mean absolute error between the estimated and observed voltage trace.\n",
    "    \n",
    "    Args:\n",
    "        v_est: The estimated voltage trace.\n",
    "        v_true: The true voltage trace.\n",
    "\n",
    "    Returns:\n",
    "        err: The mean absolute error.\n",
    "    \"\"\"\n",
    "    return # implement the mean absolute error here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a batch of parameter (randomly or on a grid)\n",
    "\n",
    "# grid-search\n",
    "gridsize = (20, 20, 20)\n",
    "\n",
    "# random search\n",
    "num_samples = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the batch of parameters\n",
    "v_samples =\n",
    "\n",
    "# compute the mean absoluteerror\n",
    "err = mean_abs_err(v_samples, v_obs)\n",
    "\n",
    "# get the top 10 results\n",
    "top_indices = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 10 results and compare them with the observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection ABC\n",
    "While using random and grid searches to sample the parameter space can yield good results, they struggle to deal with uncertainty and noisy data. Bayesian inference tries to alleviate this by infering a posterior distribution, rather than a point estimate. Unfortunately, most simulators have intractable likelihood functions however and hence we can only approximately do Bayesian inference (Approximate Bayesian Computation, ABC). One method to do this is called rejection ABC, where we approximate the posterior using samples from the simulator.\n",
    "\n",
    "### Tasks\n",
    "- Implement a rejection ABC sampler using the mean squared error as a distance metric\n",
    "- Sample the parameter space\n",
    "- Plot your top 10 parameter samples. Compare them to the previous best samples\n",
    "- Play around with the threshold to see how it affects the results\n",
    "\n",
    "### Questions\n",
    "- Are the results what you expected? Why / Why not?\n",
    "- What are different ways in which you could improve the results?\n",
    "\n",
    "_Note you can use the `mean_abs_err` function you defined before._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the rejection ABC sampler\n",
    "def rejection_abc(num_samples: int, threshold: float, distance_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray] = mean_abs_err) -> Tuple[dict[str, jnp.ndarray], jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Rejection ABC sampler.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: The number of samples to draw.\n",
    "        threshold: The threshold for the distance.\n",
    "        distance_fn: The distance function.\n",
    "    \"\"\"\n",
    "    # Sample parameters from prior\n",
    "    rng = jax.random.split(jax.random.PRNGKey(0), len(bounds))\n",
    "    samples = {k: jax.random.uniform(sub_rng, (num_samples,), minval=lb, maxval=ub) \n",
    "               for sub_rng, (k, (lb, ub)) in zip(rng, bounds.items())}\n",
    "    \n",
    "    # Simulate for all parameter samples\n",
    "    v_samples = \n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = \n",
    "    \n",
    "    # Select samples below threshold\n",
    "    accepted_indices = \n",
    "    accepted_params = \n",
    "    accepted_distances = \n",
    "    \n",
    "    return accepted_params, accepted_distances, v_samples[accepted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rejection ABC\n",
    "num_samples = 10_000\n",
    "threshold = 13  # You may need to adjust this based on your data\n",
    "accepted_params, accepted_distances, accepted_v = rejection_abc(num_samples, threshold)\n",
    "\n",
    "# Sort by distance to get the best fits\n",
    "sorted_indices = \n",
    "top_indices =  # Get top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the estimated posterior within the prior bounds using `pairplot`\n",
    "# Note the pairplot expects a torch tensor as input\n",
    "# you can learn more about `pairplot` if you run `?pairplot`\n",
    "accepted_samples = torch.tensor(np.array(list(accepted_params.values())).T)\n",
    "# plot the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top voltage traces of the top 10 parameter samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection ABC (with features)\n",
    "Rather than computing the distance on the raw voltage traces of the HH model, it can be beneficial to compute the distance on a lower dimensional representation of the data. This is computationally much easier and more robust. A common approach to reduce the dimensionality of the data is to compute so called summary features, which are able to capture more of the global structure of the data. \n",
    "\n",
    "### Tasks\n",
    "- Implement a distance metric based on summary features\n",
    "    - How could you quantify global charecteristics of the voltage trace? Think of statistical properties.\n",
    "    - Come up with your own summary features and implement them\n",
    "- Run the rejection ABC sampler again with the new distance metric\n",
    "- Plot your top 10 parameter samples. Compare them to the previous best samples\n",
    "- Play around with the threshold to see how it affects the results\n",
    "\n",
    "### Questions\n",
    "- How do the results compare to the previous results? Which ones would you consider to be better?\n",
    "- What are ways in which you could improve the results even further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_stats_np(v: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Compute summary statistics of the voltage trace.\n",
    "    \n",
    "    A feature can be anything that takes the time series as input and returns a single number.\n",
    "\n",
    "    Args:\n",
    "        v: The voltage trace.\n",
    "\n",
    "    Returns:\n",
    "        summary_stats: The summary statistics.\n",
    "    \"\"\"\n",
    "    ft1 = \n",
    "    ft2 = \n",
    "    ft3 = \n",
    "    ...\n",
    "    return jnp.hstack([ft1, ft2, ft3, ...])\n",
    "\n",
    "\n",
    "def fts_err(v_est: jnp.ndarray, v_true: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Compute a feature based distance.\n",
    "\n",
    "    ```\n",
    "    fts_est = compute_summary_stats(v_est)\n",
    "    fts_true = compute_summary_stats(v_true)\n",
    "\n",
    "    fts_err = sum(jnp.abs(fts_est - fts_true))\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        v_est: The estimated voltage trace.\n",
    "        v_true: The true voltage trace.\n",
    "\n",
    "    Returns:\n",
    "        ft_err: The feature based distance.\n",
    "    \"\"\"\n",
    "    # implement the feature based distance here\n",
    "    return fts_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rejection ABC\n",
    "num_samples = 10_000\n",
    "threshold = 25  # You may need to adjust this based on your data\n",
    "accepted_params, accepted_distances, accepted_v = rejection_abc(num_samples, threshold, fts_err)\n",
    "\n",
    "# Sort by distance to get the best fits\n",
    "sorted_indices = \n",
    "top_indices =   # Get top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the estimated posterior within the prior bounds using `pairplot`\n",
    "# Note the pairplot expects a torch tensor as input\n",
    "accepted_samples = torch.tensor(np.array(list(accepted_params.values())).T)\n",
    "# plot the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10 simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Posterior Estimation\n",
    "\n",
    "While rejection ABC provides a more principled approach than grid or random searches, it can become inefficient as the dimensionality of the parameter space increases or when the simulator is computationally expensive. Neural Posterior Estimation (NPE) addresses these limitations by using neural networks to learn an approximation of the posterior distribution directly from simulation data.\n",
    "\n",
    "NPE leverages the power of conditional density estimation to build a model that maps from observed data to the corresponding posterior distribution over parameters. This approach often requires fewer simulations than traditional ABC methods and can capture complex posterior distributions more effectively.\n",
    "\n",
    "### Tasks\n",
    "- Implement NPE using the `sbi` package, use the lower dimensional summary features as input. For details see the [documentation](https://sbi-dev.github.io/sbi/0.22/)\n",
    "    - Use one round of `SNPE` for that\n",
    "    - use a MDN as a density estimator (run `?SNPE` for details)\n",
    "    - reimplement the `compute_summary_stats` function to work with torch tensors\n",
    "- plot the posterior\n",
    "- compute the MAP estimate\n",
    "\n",
    "### Questions\n",
    "- Under what circumstances might NPE be preferable to rejection ABC, and vice versa?\n",
    "- How do the posterior estimates from NPE compare to those from rejection ABC in terms of accuracy and efficiency?\n",
    "\n",
    "_Note: the `sbi` package is based on pytorch and hence expects `torch.tensors` as inputs and outputs. These work similarly to numpy arrays and also come with similar functions, i.e `torch.sum` etc. Use the [documentation](https://sbi-dev.github.io/sbi/0.22/) to answer your questions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Uniform prior for the parameters\n",
    "prior = BoxUniform() # use the parameter bounds\n",
    "\n",
    "# reimplement the summary statistics function to work with torch tensors or \n",
    "# wrap the numpy function to work with torch tensors\n",
    "def compute_summary_stats_torch(x: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"Compute summary statistics of the voltage trace.\n",
    "    \n",
    "    Args:\n",
    "        x: The voltage trace.\n",
    "\n",
    "    Returns:\n",
    "        summary_stats: The summary statistics.\n",
    "    \"\"\"\n",
    "    return # implement the summary statistics here\n",
    "\n",
    "# simulator function for sbi\n",
    "def simulate_for_sbi(theta: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"Simulate the HH model for the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        theta: The parameters of the HH model.\n",
    "\n",
    "    Returns:\n",
    "        v: The voltage trace.\n",
    "    \"\"\"\n",
    "    # since jaxley expects jax arrays and sbi uses torch tensors\n",
    "    # we need to convert theta back and forth\n",
    "    theta = theta.numpy()\n",
    "    theta = {k:v for k,v in zip(bounds.keys(), theta.T)}\n",
    "    return torch.tensor(parallel_simulate(theta)).to(torch.float32)\n",
    "\n",
    "# draw samples from the prior, simulate and compute summary statistics\n",
    "theta_train = \n",
    "v_train = \n",
    "x_train = \n",
    "x_obs = \n",
    "\n",
    "# run NPE (one round of SNPE)\n",
    "snpe = # set up SNPE\n",
    "infer = snpe.append_simulations(theta_train, x_train)\n",
    "estimator = infer.train()\n",
    "posterior = # build posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition the posterior on the observed data\n",
    "posterior.set_default_x(x_obs)\n",
    "\n",
    "# sample from the posterior\n",
    "samples = \n",
    "\n",
    "# get the top 10 results and simulate them\n",
    "top_indices = \n",
    "v_top = \n",
    "\n",
    "# compute the MAP estimate\n",
    "theta_map = \n",
    "print(f\"MAP estimate: gNa={theta_map[:, 0].item():.3f}, gK={theta_map[:, 1].item():.3f}, gLeak={theta_map[:, 2].item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the estimated posterior within the prior bounds using `pairplot`and the MAP estimate\n",
    "# Note the pairplot expects a torch tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 10 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent via Backpropagation of Error\n",
    "While sampling-based methods like ABC and Neural Posterior Estimation provide distributional estimates of parameters, optimization approaches offer an alternative strategy when we primarily seek point estimates. This can be especially useful when the simulator is computationally expensive but gradients over the simulator parameters are available, since gradient-based optimization usually requires many fewer evaluations of the simulator compared to sampling-based methods.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "- Optimize parameters directly on the time series data using gradient descent\n",
    "- Bonus: Optimize using summary statistics as an alternative approach\n",
    "\n",
    "### Questions\n",
    "- How do the results compare to the ones from SBI methods?\n",
    "- How do the results from the feature based approach compare to the ones from the mean squared error loss?\n",
    "- What are ways to make the optimization process more robust to local minima?\n",
    "\n",
    "_Note: to ensure that the parameters are optimized within their bounds, we use use sigmoid to transform between the interval [a,b] <-> [-inf,inf]. You can use the [documentation](https://jaxley.readthedocs.io/en/latest/index.html) and [tutorials](https://jaxley.readthedocs.io/en/latest/tutorials.html) to help you answer the questions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = jx.ParamTransform({k:SigmoidTransform(lb[k], ub[k]) for k in bounds.keys()})\n",
    "\n",
    "# define the loss function\n",
    "def loss(params: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Compute the mean absolute error between the estimated and observed voltage trace.\n",
    "    \n",
    "    Args:\n",
    "        params: The parameters of the HH model.\n",
    "\n",
    "    Returns:\n",
    "        loss: The mean absolute error.\n",
    "    \"\"\"\n",
    "    params = transform.forward(params) # transform the parameters\n",
    "    v_est = \n",
    "    err = \n",
    "    return err\n",
    "\n",
    "diff = jit(value_and_grad(loss)) # take the gradient of the loss function (and use jit to speed up the computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and initialize the parameters (you can play around with the learning rate, number of epochs, etc.)\n",
    "lr = 1e-1\n",
    "max_epochs = 5000\n",
    "init_params = {\"HH_gNa\": 0.12, \"HH_gK\": 0.3, \"HH_gLeak\": 0.0003}\n",
    "opt_params = transform.inverse(init_params)\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(opt_params)\n",
    "\n",
    "# run the optimization\n",
    "\n",
    "# loop over the epochs\n",
    "    loss_val, grad_val = # obtain the gradient and value of the loss function\n",
    "\n",
    "    updates, opt_state = optimizer.update(grad_val, opt_state)\n",
    "    opt_params = optax.apply_updates(opt_params, updates)\n",
    "    \n",
    "# get the final parameters\n",
    "final_params = transform.forward(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the final parameters\n",
    "\n",
    "# plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
